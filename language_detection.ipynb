{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('File Path to Dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk as nltk\n",
    "import string as str\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "import csv\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from tensorflow import keras\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"File Path to Dataset\")\n",
    "pd.set_option('display.max_colwidth',200)\n",
    "data.head(10337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_a = data[\"Text\"]\n",
    "print(column_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()\n",
    "df.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.countplot(x='Language',data=data,palette='Set1')\n",
    "plt.title('Target Labels',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct=str.punctuation\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'][3500:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #removing punctuation\n",
    "def remove_punct(tokens):\n",
    "    filtered_sentence_stopwords =\"\"\n",
    "    PUNCT = punctuation\n",
    "    PUNCT+='\"\"'\n",
    "    PUNCT+='``'\n",
    "    PUNCT+=\"''\"+\"«\"+\"»\"+\"؟\"\n",
    "\n",
    "    for w in tokens:\n",
    "        if w not in PUNCT:\n",
    "            filtered_sentence_stopwords+=w\n",
    "    return filtered_sentence_stopwords\n",
    "df['text_nopunc']=data['Text'].apply(lambda x:remove_punct(x.lower()))\n",
    "df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_nopunc'][1424:9450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens=word_tokenize(text)\n",
    "    return tokens\n",
    "df['text_tokniz']=df['text_nopunc'].apply(lambda x : tokenize (x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = nltk.corpus.stopwords.words('english')\n",
    "stop_words_ge = stopwords.words('german')\n",
    "stop_words_fr = stopwords.words('french')\n",
    "stop_words_it = stopwords.words('italian')\n",
    "stop_words_po = stopwords.words('portuguese')\n",
    "stop_words_ru = stopwords.words('russian')\n",
    "stop_words_sp = stopwords.words('spanish')\n",
    "stop_words_sw = stopwords.words('swedish')\n",
    "stop_words_da = stopwords.words('danish')\n",
    "stop_words_du = stopwords.words('dutch')\n",
    "stop_words_ar = stopwords.words('arabic')\n",
    "stop_words_tu = stopwords.words('turkish')\n",
    "stop_words_gr = stopwords.words('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tok_l,stopwords):\n",
    "    filtered_sentence =[]\n",
    "    for w in tok_l:\n",
    "        if w not in stopwords:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filtered_sentence_stopwords']=df['text_tokniz'].apply(lambda x : remove_stopwords(x,stop_words_en))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_ge))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_fr))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_ru))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_sw))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_da))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_du))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_ar))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_tu))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_it))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_po))\n",
    "df['filtered_sentence_stopwords']=df['filtered_sentence_stopwords'].apply(lambda x : remove_stopwords(x,stop_words_sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=df['text_nopunc']\n",
    "l=df['Language']\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {0:\"English\",\n",
    "1:\"French\",\n",
    "2:\"Spanish\",\n",
    "3:\"Portugeese\",\n",
    "4:\"Italian\",\n",
    "5:\"Russian\",\n",
    "6:\"Sweedish\",\n",
    "7:\"Malayalam\",\n",
    "8:\"Dutch\",\n",
    "9:\"Arabic\",\n",
    "10:\"Turkish\",\n",
    "11:\"German\",\n",
    "12:\"Tamil\",\n",
    "13:\"Danish\",\n",
    "14:\"Kannada\",\n",
    "15:\"Greek\",\n",
    "16:\"Hindi\"} \n",
    "def getcode(n) : \n",
    "    for x , y in code.items() : \n",
    "        if n == y : \n",
    "            return x \n",
    "print(getcode(\"Sweedish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for i in range(10337):\n",
    "    class_num=getcode(l[i])\n",
    "    training_data.append([f[i], class_num]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data[0]))\n",
    "print(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for text,label in training_data:\n",
    "    x.append(text)\n",
    "    y.append(label)\n",
    "for i in range(10):\n",
    "    print(x[i])\n",
    "    print(getcodel(y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "features_tfidf=tfidf.fit_transform(x).toarray()\n",
    "print(features_tfidf.shape)\n",
    "print('Sparse Matrix :\\n',features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=CountVectorizer()\n",
    "features_count=count.fit_transform(x).toarray()\n",
    "print(features_count.shape)\n",
    "print('Sparse Matrix :\\n',features_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram=CountVectorizer(ngram_range=(2,2))\n",
    "features_n_gram=n_gram.fit_transform(x)\n",
    "print(features_n_gram.shape)\n",
    "print('Sparse Matrix :\\n',features_n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features_n_gram,y,test_size=0.1, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_detect_model = MultinomialNB().fit(x_train, y_train)\n",
    "pred_test_MNB = language_detect_model.predict(x_test)\n",
    "precision = precision_score(y_test, pred_test_MNB,pos_label='positive',average='micro')\n",
    "recall = recall_score(y_test, pred_test_MNB,pos_label='positive',average='micro')\n",
    "accuracy = accuracy_score(y_test, pred_test_MNB)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
    "print(confusion_matrix(y_test,pred_test_MNB))\n",
    "print (classification_report(y_test, pred_test_MNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_tfidf,y,test_size=0.1, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_detect_model = MultinomialNB().fit(x_train, y_train)\n",
    "pred_test_MNB = language_detect_model.predict(x_test)\n",
    "precision = precision_score(y_test, pred_test_MNB,pos_label='positive',average='micro')\n",
    "recall = recall_score(y_test, pred_test_MNB,pos_label='positive',average='micro')\n",
    "accuracy = accuracy_score(y_test, pred_test_MNB)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
    "print(confusion_matrix(y_test,pred_test_MNB))\n",
    "print (classification_report(y_test, pred_test_MNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_count,y,test_size=0.1, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_detect_model = MultinomialNB().fit(x_train, y_train)\n",
    "pred_test_MNB = language_detect_model.predict(x_test)\n",
    "precision = precision_score(y_test, pred_test_MNB,pos_label='positive',average='micro')\n",
    "recall = recall_score(y_test, pred_test_MNB,pos_label='positive',average='micro')\n",
    "accuracy = accuracy_score(y_test, pred_test_MNB)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
    "print(confusion_matrix(y_test,pred_test_MNB))\n",
    "print (classification_report(y_test, pred_test_MNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"également préciser\"\n",
    "x = count.transform([text])\n",
    "pred = language_detect_model.predict(x)\n",
    "print(getcodel(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"من\"\n",
    "x = count.transform([text])\n",
    "pred = language_detect_model.predict(x)\n",
    "print(getcodel(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('count.pickle', 'wb') as handle:\n",
    "    pickle.dump(count,handle)\n",
    "# save the model to disk\n",
    "filename = 'MultinomialNB.sav'\n",
    "pickle.dump(language_detect_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('count.pickle', 'rb') as handle:\n",
    "    count = pickle.load(handle)\n",
    "\n",
    "with open('MultinomialNB.sav', 'rb') as handle:\n",
    "    language_detect_model = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acbd9ef49d24c81838bcab682cd0fe925bceac95fddda1cfd78f89ac06614849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
